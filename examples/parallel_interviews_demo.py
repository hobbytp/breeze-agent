"""
æ¼”ç¤ºå¹¶è¡Œè®¿è°ˆåŠŸèƒ½çš„ç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨é…ç½®é€‰é¡¹æ¥æ§åˆ¶è®¿è°ˆæ¨¡å¼ï¼š
- ä¸²è¡Œæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰ï¼šç¼–è¾‘å™¨ä¾æ¬¡è¿›è¡Œè®¿è°ˆ
- å¹¶è¡Œæ¨¡å¼ï¼šæ‰€æœ‰ç¼–è¾‘å™¨åŒæ—¶è¿›è¡Œè®¿è°ˆï¼Œæå‡æ€§èƒ½
"""

import asyncio
import sys
import os
import traceback

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from web_research_graph.graph import graph
from web_research_graph.state import State, TopicValidation
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import HumanMessage
from IPython.display import display, Markdown

async def demo_serial_interviews():
    """æ¼”ç¤ºä¸²è¡Œè®¿è°ˆæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰"""
    print("=== ä¸²è¡Œè®¿è°ˆæ¨¡å¼æ¼”ç¤º ===")
    
    # ä¸²è¡Œæ¨¡å¼é…ç½®ï¼ˆé»˜è®¤ï¼‰
    config = {
        "configurable": {
            "parallel_interviews": False,  # ä¸²è¡Œæ¨¡å¼
            "max_turns": 2,  # æ¯ä¸ªç¼–è¾‘å™¨æœ€å¤š2è½®å¯¹è¯
        }
    }
    
    # è¾“å…¥çŠ¶æ€
    input_state = {
        "messages": [],
        "topic": TopicValidation(
            is_valid=True, 
            topic="äººå·¥æ™ºèƒ½çš„å‘å±•å†å²", 
            message="æœ‰æ•ˆçš„ä¸»é¢˜"
        ).model_dump()
    }
    
    print(f"ä¸»é¢˜: {input_state['topic']['topic']}")
    print(f"è®¿è°ˆæ¨¡å¼: ä¸²è¡Œ")
    print("å¼€å§‹æ‰§è¡Œ...")
    
    # æ³¨æ„ï¼šè¿™åªæ˜¯æ¼”ç¤ºä»£ç ç»“æ„ï¼Œå®é™…è¿è¡Œéœ€è¦å®Œæ•´çš„LLMé…ç½®
    # result = await graph.ainvoke(input_state, config=config)
    print("ä¸²è¡Œæ¨¡å¼ï¼šç¼–è¾‘å™¨å°†ä¾æ¬¡è¿›è¡Œè®¿è°ˆ")

async def demo_parallel_interviews():
    """æ¼”ç¤ºå¹¶è¡Œè®¿è°ˆæ¨¡å¼"""
    print("\n=== å¹¶è¡Œè®¿è°ˆæ¨¡å¼æ¼”ç¤º ===")
    
    my_custom_params = {
        "fast_llm_model": "openai/gpt-4o-mini",
        "long_context_model": "openai/gpt-4o",
        "max_editors": 2,
        "max_search_results": 1,  # ä½ ä¹Ÿå¯ä»¥è¦†ç›–éæ¨¡å‹å‚æ•°
        "parallel_interviews": True,  # å¹¶è¡Œæ¨¡å¼
        "max_parallel_interviews": 3,  # æœ€å¤š3ä¸ªå¹¶å‘è®¿è°ˆ
        "max_turns": 2,  # æ¯ä¸ªç¼–è¾‘å™¨æœ€å¤š2è½®å¯¹è¯
        "system_prompt": "You are a helpful assistant that can help with research tasks."  # ç”šè‡³å¯ä»¥è¦†ç›–ç³»ç»Ÿæç¤º
    }

    # åˆ›å»ºä¸€ä¸ª RunnableConfig å®ä¾‹
    run_config = RunnableConfig(
        configurable=my_custom_params
    )
    
    # ä½¿ç”¨æ­£ç¡®çš„LangChainæ¶ˆæ¯æ ¼å¼
    topic_text = "please study the latest developments regarding Multiple Agent in artificial intelligence."
    input_data = {
        "messages": [HumanMessage(content=topic_text)]
    }
    print("æ­£åœ¨å¼‚æ­¥è°ƒç”¨ç½‘ç»œç ”ç©¶å›¾...")
    
    print(f"ä¸»é¢˜: {topic_text}")
    print(f"è®¿è°ˆæ¨¡å¼: å¹¶è¡Œ")
    print(f"æœ€å¤§å¹¶å‘æ•°: {my_custom_params['max_parallel_interviews']}")
    print("å¼€å§‹æ‰§è¡Œ...")

    try:
        result = await graph.ainvoke(input=input_data, config=run_config)
        print("\n--- ç ”ç©¶å®Œæˆ ---")
        
        # é¦–å…ˆæ£€æŸ¥resultæ˜¯å¦ä¸ºNone
        if result is None:
            print("å›¾æ‰§è¡Œè¿”å›äº†Noneç»“æœ")
            print("è¿™å¯èƒ½æ˜¯å› ä¸ºå·¥ä½œæµç¨‹åœ¨æŸä¸ªèŠ‚ç‚¹ä¸­æ–­äº†")
            return
            
        # æ£€æŸ¥ 'article' æ˜¯å¦åœ¨ç»“æœä¸­å¹¶æ˜¾ç¤ºå®ƒ
        if "article" in result and result["article"]:
            print("\n--- ç ”ç©¶ç»“æœ ---")
            print("ç”Ÿæˆçš„æ–‡ç« :")
            try:
                display(Markdown(result["article"]))
            except:
                # å¦‚æœdisplayå¤±è´¥ï¼Œç›´æ¥æ‰“å°
                print(result["article"])
        else:
            print("æœªèƒ½ç”Ÿæˆæ–‡ç« ã€‚å®Œæ•´çš„è¿”å›ç»“æœå¦‚ä¸‹:")
            print(f"ç»“æœç±»å‹: {type(result)}")
            print(f"ç»“æœå†…å®¹: {result}")
            
    except Exception as e:
        print(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("é”™è¯¯è¯¦æƒ…:")
        traceback.print_exc()
        print("è¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºç¨‹åºï¼Œå®é™…è¿è¡Œéœ€è¦é…ç½®APIå¯†é’¥ç­‰ç¯å¢ƒå˜é‡")

def compare_modes():
    """å¯¹æ¯”ä¸¤ç§æ¨¡å¼çš„ç‰¹ç‚¹"""
    print("\n=== æ¨¡å¼å¯¹æ¯” ===")
    print("ä¸²è¡Œæ¨¡å¼:")
    print("  âœ“ å‘åå…¼å®¹ï¼Œç¨³å®šå¯é ")
    print("  âœ“ èµ„æºæ¶ˆè€—è¾ƒä½")
    print("  âœ— è€—æ—¶è¾ƒé•¿ï¼ˆç¼–è¾‘å™¨ä¾æ¬¡è®¿è°ˆï¼‰")
    
    print("\nå¹¶è¡Œæ¨¡å¼:")
    print("  âœ“ æ€§èƒ½æ˜¾è‘—æå‡ï¼ˆ3-5å€é€Ÿåº¦æå‡ï¼‰")
    print("  âœ“ æ‰€æœ‰ç¼–è¾‘å™¨åŒæ—¶å·¥ä½œ")
    print("  âœ“ å¯é…ç½®å¹¶å‘æ•°é‡æ§åˆ¶APIé™é¢")
    print("  âœ— èµ„æºæ¶ˆè€—è¾ƒé«˜")
    print("  âœ— éœ€è¦æ³¨æ„APIé€Ÿç‡é™åˆ¶")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¹¶è¡Œè®¿è°ˆåŠŸèƒ½æ¼”ç¤º")
    print("=" * 50)
    
    # await demo_serial_interviews()
    await demo_parallel_interviews()
    # compare_modes()
    
    print("\n=== ä½¿ç”¨å»ºè®® ===")
    print("1. å¼€å‘å’Œæµ‹è¯•é˜¶æ®µï¼šä½¿ç”¨ä¸²è¡Œæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰")
    print("2. ç”Ÿäº§ç¯å¢ƒéœ€è¦é«˜æ€§èƒ½ï¼šå¯ç”¨å¹¶è¡Œæ¨¡å¼")
    print("3. æ ¹æ®APIé™é¢è°ƒæ•´ max_parallel_interviews")
    print("4. ç›‘æ§APIä½¿ç”¨æƒ…å†µï¼Œé¿å…è¶…å‡ºé™é¢")

if __name__ == "__main__":
    asyncio.run(main()) 